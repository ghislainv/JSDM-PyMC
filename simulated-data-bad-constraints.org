# -*- mode: org -*-
# -*- coding: utf-8 -*-
# -*- org-src-preserve-indentation: t; org-edit-src-content: 0; -*-

# ==============================================================================
# author          :Ghislain Vieilledent
# email           :ghislain.vieilledent@cirad.fr, ghislainv@gmail.com
# web             :https://ecology.ghislainv.fr
# license         :GPLv3
# ==============================================================================

#+title: Simulated data with bad constraints
#+author: Ghislain Vieilledent
#+email: ghislain.vieilledent@cirad.fr

#+LANGUAGE: en
#+TAGS: Blog(B) noexport(n) Stats(S)
#+TAGS: Ecology(E) R(R) OrgMode(O) Python(P)
#+OPTIONS: H:3 num:t toc:t \n:nil @:t ::t |:t ^:{} -:t f:t *:t <:t tex:t
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

# HTML themes
#+HTML_DOCTYPE: html5
#+OPTIONS: html-postamble:nil html-style:nil html-scripts:nil html5-fancy:t
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="style/worg.css"/>

# Image style in HTML
# #+HTML_HEAD_EXTRA: <style>img {width: 900px;}</style>

# For math display
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{unicode-math}

#+PROPERTY: header-args :eval never-export

* Installing PyMC in a Python virtual environment

The best way to install the package is to create a Python virtual environment, for example using =conda=. You first need to have [[https://docs.conda.io/en/latest/miniconda.html][miniconda3]] installed. Then, create a [[https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html][conda environment]] and install the PyMC package with the following commands:

#+begin_src shell :eval no
conda create --name JSDM-PyMC -c conda-forge python=3.9
conda activate JSDM-PyMC
conda install -c conda-forge mamba
mamba install -c conda-forge "pymc>=4"
conda install -c conda-forge scikit-learn flake8 jedi tabular
#+end_src

To deactivate and delete the conda environment, use the following commands:

#+begin_src shell :eval no
conda deactivate
conda env remove --name JSDM-PyMC
#+end_src

#+RESULTS:

* Import libraries

#+begin_src python :tangle yes :comments both :results output :session :exports both
import os
import sys

import arviz as az
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import cloudpickle
import pymc as pm
import pytensor.tensor as pt
from tabulate import tabulate

print(f"Running on PyMC v{pm.__version__}")
#+end_src

#+RESULTS:
: Running on PyMC v5.0.2

* Functions

#+begin_src python :tangle yes :comments both :results output :session :exports both
def inv_logit(p):
    return 1. / (1. + np.exp(-p))
#+end_src

#+RESULTS:

* Simulating data

#+begin_src python :tangle yes :comments both :results output :session :exports both
# Set seed for repeatability
SEED = 1234
rng = np.random.default_rng(SEED)

# Number of sites and species
n_sites = 100
n_species = 30
# Number of latent variables
n_q = 2

# Ecological variables
Int = np.array([1] * n_sites)
x1 = rng.standard_normal(size=n_sites)
x2 = rng.standard_normal(size=n_sites)
X = np.array([Int, x1, x2]).transpose()
print("X.shape:")
print(X.shape)
print("\nX[:5,]:")
print(X[:5,])
# Number of explicative variables
n_p = X.shape[1]

# Latent variables
w1 = rng.standard_normal(size=n_sites)
w2 = rng.standard_normal(size=n_sites)
W_target = np.array([w1, w2]).transpose()
print("\nW.target.shape:")
print(W_target.shape)
print("\nW_target[:5,]:")
print(W_target[:5,])
# (Check that W_target are independant)
R_W = np.corrcoef(W_target, rowvar=False)
print("\nR_W:")
print(R_W)

# Fixed species effects beta
beta_target = rng.uniform(-1, 1, n_p*n_species).reshape(n_species, n_p)

# Factor loading lambda
lambda_target = rng.uniform(-1, 1, n_q*n_species).reshape(n_species, n_q)
# Higher variation on axis 0
lambda_target[:, 0] = lambda_target[:, 0] * 2
# Constraints on lambda
lambda_target[0, 1] = 0
lambda_target[np.arange(n_q), np.arange(n_q)] =  np.array([-0.1] * n_q)
lambda_target[2, 0] = 3
lambda_target[2, 1] = 0
lambda_target[3, 0] = rng.uniform(-0.1, 0.1)
lambda_target[3, 1] = 2
print("\nlambda_target.shape:")
print(lambda_target.shape)
print("\nlambda_target[:5,]:")
print(lambda_target[:5,])

# Variance of random site effects 
sigma_alpha_target = 0.5
# Random site effects
alpha_target = rng.normal(loc=0, scale=sigma_alpha_target, size=n_sites)

# Probabilities
Xbeta_target = np.matmul(X, beta_target.transpose())
Wlambda_target = np.matmul(W_target, lambda_target.transpose()) 
logit_theta_target = alpha_target[:, np.newaxis] + Xbeta_target + Wlambda_target
theta_target = inv_logit(logit_theta_target)

# Simulated occurrences
Y = rng.binomial(n=1, p=theta_target)
print("\nY.shape:")
print(Y.shape)

# Save data-set
out_dir = "outputs/simulated-data-bad-constraints/"
with open(out_dir + "data.pkl", "wb") as f:
     data_dump = cloudpickle.dumps({"Y": Y, "X": X})
     f.write(data_dump)
#+end_src

#+RESULTS:
#+begin_example
X.shape:
(100, 3)

X[:5,]:
[[ 1.         -1.60383681  2.25392546]
 [ 1.          0.06409991  0.1616142 ]
 [ 1.          0.7408913   0.83377881]
 [ 1.          0.15261919 -1.58010947]
 [ 1.          0.86374389  1.01058529]]

W.target.shape:
(100, 2)

W_target[:5,]:
[[ 0.73118867 -1.18459707]
 [-0.22964706 -0.35451603]
 [ 2.14411198  1.36731227]
 [ 0.39714586  1.70012206]
 [ 0.15946658 -1.8795888 ]]

R_W:
[[1.         0.02136166]
 [0.02136166 1.        ]]

lambda_target.shape:
(30, 2)

lambda_target[:5,]:
[[-0.1         0.        ]
 [ 0.14983372 -0.1       ]
 [ 3.          0.        ]
 [ 0.07563471  2.        ]
 [ 0.38208208 -0.88684922]]

Y.shape:
(100, 30)
#+end_example

Histogram of Wlambda.

#+begin_src python :tangle yes :comments both :results file :session :exports both
ofile = os.path.join(out_dir, "hist_Wlambda.png")
fig = plt.figure()
plt.hist(Wlambda_target.flatten(), bins=20)
fig.savefig(ofile)
ofile
#+end_src

#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/hist_Wlambda.png]]

* Model

#+begin_src python :tangle yes :comments both :results output :session :exports both
HALFNORMAL_SCALE = 1. / np.sqrt(1. - 2. / np.pi)
#+end_src

#+RESULTS:

We create a function to expand a packed block triangular matrix. Triangular matrices can be stored with better space efficiency by storing the non-zero values in a one-dimensional array. This function is an adaptation of =pm.expand.packed.triangular=.

#+begin_src python :tangle yes :comments both :results output :session :exports both
def expand_packed_block_triangular(n_species, n_q, packed, diag=None, mtype="pytensor"):
    # like pm.expand_packed_triangular, but with n_species > n_q.
    assert mtype in {"pytensor", "numpy"}
    assert n_species >= n_q

    def set_(M, i_, v_):
        if mtype == "pytensor":
            return pt.set_subtensor(M[i_], v_)
        M[i_] = v_
        return M

    out = pt.zeros((n_species, n_q), dtype=float) if mtype == "pytensor" else np.zeros((n_species, n_q), dtype=float)
    if diag is None:
        idxs = np.tril_indices(n_species, m=n_q)
        out = set_(out, idxs, packed)
    else:
        idxs = np.tril_indices(n_species, k=-1, m=n_q)
        out = set_(out, idxs, packed)
        idxs = (np.arange(n_q), np.arange(n_q))
        out = set_(out, idxs, diag)
    return out
#+end_src

#+RESULTS:

We define another function which creates a diagonal matrix with positive values on the diagonal.

#+begin_src python :tangle yes :comments both :results output :session :exports both
def makeLambda(n_species, n_q, dim_names):
    # Number of non-zeros factor loadings
    n_L_packed = int(n_species * n_q - n_q * (n_q - 1) / 2 - n_q)
    # Diagonal matrix
    L_diag = pm.HalfNormal("L_diag", sigma=HALFNORMAL_SCALE, shape=n_q)
    # Packed Lambda
    L_packed = pm.Normal("L_packed", mu=0, sigma=1, shape=n_L_packed)
    L = expand_packed_block_triangular(n_species, n_q, L_packed, diag=pt.ones(n_q))
    Lambda = pm.Deterministic("Lambda", pt.dot(L, pt.diag(L_diag)), dims=dim_names)
    return Lambda
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results output :session :exports both
with pm.Model() as model:
    # Hyperpriors
    sigma_alpha = pm.HalfNormal("sigma_alpha", sigma=1.0)
    # Priors
    # Site random effect
    alpha = pm.Normal("alpha", mu=0, sigma=sigma_alpha, shape=n_sites, dims="sites")
    # Latent variables
    W = pm.Normal("W", mu=0, sigma=1, shape=(n_sites, n_q), dims=("sites", "latent_axis"))
    # Species effects
    beta = pm.Normal("beta", mu=0, sigma=1, shape=(n_species, n_p), dims=("species", "fixed_effects"))
    # Factor loadings with constraints
    Lambda = makeLambda(n_species, n_q, ("species", "latent_axis"))
    # Likelihood
    Xbeta = pm.math.dot(X, beta.transpose())
    Wlambda = pm.math.dot(W, Lambda.transpose()) 
    logit_theta = alpha[:, np.newaxis] + Xbeta + Wlambda
    obs = pm.Bernoulli("obs", logit_p=logit_theta, observed=Y)
#+end_src

#+RESULTS:

Parameters for MCMC sampling:

#+begin_src python :tangle yes :comments both :results output :session :exports both
CORES = 2
SAMPLE_KWARGS = {
    'draws': 1000,
    'cores': CORES,
    'init': 'auto',
    'tune': 1000,
    'random_seed': [SEED + i for i in range(CORES)]
}
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results silent :session :exports code
# Inference
with model:
    trace = pm.sample(**SAMPLE_KWARGS)
#+end_src

Save model with cloudpickle (cf. [[https://github.com/pymc-devs/pymc/issues/5886][link]]).

#+begin_src python :tangle yes :comments both :results silent :session :exports both
out_dir = "outputs/simulated-data-bad-constraints/"
with open(out_dir + "model_trace.pkl", "wb") as f:
     model_trace_dump = cloudpickle.dumps({'model': model, 'trace': trace})
     f.write(model_trace_dump)
#+end_src

Then, model results can be loaded with the following code:

#+begin_src python :tangle yes :comments both :eval no :exports code
f = open(out_dir + "model_trace.pkl", "rb")
model_trace = cloudpickle.loads(f.read())
#+end_src

* Convergence and model performance

** Plotting traces

#+begin_src python :tangle yes :comments both :results file :session :exports both
ofile = out_dir + "trace.png"
with model:
    axes = az.plot_trace(trace,
                         var_names=["alpha", "beta",
                                    "sigma_alpha"])
fig = axes.ravel()[0].figure
fig.savefig(ofile)
ofile
#+end_src

#+ATTR_HTML: :width 900
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/trace.png]]

** Parameter estimates.

#+begin_src python :tangle yes :comments both :results output :session :exports both
with model:
    summary = az.summary(trace,
                         var_names=["alpha", "beta",
                                    "sigma_alpha"], round_to=2)
summary.to_csv(out_dir + "model_summary.txt")
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results output :session :exports code
with model:
    alpha_est = az.summary(trace, var_names=["alpha"], round_to=2)
    beta_est = az.summary(trace, var_names=["beta"], round_to=2)
    lambda_est = az.summary(trace, var_names=["Lambda"], round_to=2)
    lambda_0_est = az.summary(trace,
                              var_names=["Lambda"],
                              coords={"latent_axis": [0]},
                              round_to=2)
    lambda_1_est = az.summary(trace,
                              var_names=["Lambda"],
                              coords={"latent_axis": [1]},
                              round_to=2)
    W_est = az.summary(trace, var_names=["W"], round_to=2)
    W_0_est = az.summary(trace, var_names=["W"],
                         coords={"latent_axis": [0]},
                         round_to=2)
    W_1_est = az.summary(trace, var_names=["W"],
                         coords={"latent_axis": [1]},
                         round_to=2)
#+end_src

#+RESULTS:
: /home/ghislain/.pyenv/versions/miniconda3-latest/envs/JSDM-PyMC/lib/python3.9/site-packages/arviz/stats/diagnostics.py:584: RuntimeWarning: invalid value encountered in scalar divide
:   (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)
: /home/ghislain/.pyenv/versions/miniconda3-latest/envs/JSDM-PyMC/lib/python3.9/site-packages/arviz/stats/diagnostics.py:584: RuntimeWarning: invalid value encountered in scalar divide
:   (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)

# This warning is due to the constraints lambda[0, 1]=0. No stats can be computed for this constant parameter.

#+RESULTS:

** Traces for constrained parameters

*** Factor loadings on the diagonal

#+begin_src python :tangle yes :comments both :results file :session :exports both
ofile = out_dir + "trace_lambda_00.png"
with model:
    axes = az.plot_trace(trace,
                         var_names=["Lambda"],
                         coords={"species": [0],
                                 "latent_axis": [0]})
fig = axes.ravel()[0].figure
fig.savefig(ofile)
ofile
#+end_src

#+ATTR_HTML: :width 900
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/trace_lambda_00.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports both
ofile = out_dir + "trace_lambda_11.png"
with model:
    axes = az.plot_trace(trace,
                         var_names=["Lambda"],
                         coords={"species": [1],
                                 "latent_axis": [1]})
fig = axes.ravel()[0].figure
fig.savefig(ofile)
ofile
#+end_src

#+ATTR_HTML: :width 900
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/trace_lambda_11.png]]

For these two lambdas, the MCMCs do not converge and samples are concentrated around the zero values, the closest positive value to the target values of -0.1.

#+begin_src python :tangle yes :comments both :results value raw :session :exports both
lambda_diag = lambda_est.loc[["Lambda[0, 0]", "Lambda[1, 1]"], ["mean", "sd", "r_hat"]]
lambda_diag["target_value"] = [lambda_target[0, 0], lambda_target[1, 1]]
tabulate(lambda_diag, headers="keys", tablefmt="orgtbl", showindex=True)
#+end_src

#+RESULTS:
|              | mean |   sd | r_hat | target_value |
|--------------+------+------+-------+--------------|
| Lambda[0, 0] | 0.68 | 0.13 |     1 |         -0.1 |
| Lambda[1, 1] | 0.99 | 0.15 |     1 |         -0.1 |


*** Species with high factor loadings

#+begin_src python :tangle yes :comments both :results file :session :exports both
ofile = out_dir + "trace_lambda_20.png"
with model:
    axes = az.plot_trace(trace,
                         var_names=["Lambda"],
                         coords={"species": [2],
                                 "latent_axis": [0]})
fig = axes.ravel()[0].figure
fig.savefig(ofile)
ofile
#+end_src

#+ATTR_HTML: :width 900
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/trace_lambda_20.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports both
ofile = out_dir + "trace_lambda_31.png"
with model:
    axes = az.plot_trace(trace,
                         var_names=["Lambda"],
                         coords={"species": [3],
                                 "latent_axis": [1]})
fig = axes.ravel()[0].figure
fig.savefig(ofile)
ofile
#+end_src

#+ATTR_HTML: :width 900
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/trace_lambda_31.png]]

For species with high factor loadings, the MCMCs do not converge (r_hat >> 1) and oscillate between positive and negative values (bimodal distributions) because the sign of the factor loadings are not correctly set by the constraints on the diagonal. The mean parameter estimates end up being close to zero while the target parameter values are far from zero (values 2 and -2).

#+begin_src python :tangle yes :comments both :results value raw :session :exports both
lambda_high = lambda_est.loc[["Lambda[2, 0]", "Lambda[3, 1]"], ["mean", "sd", "r_hat"]]
lambda_high["target_value"] = [lambda_target[2, 0], lambda_target[3, 1]]
tabulate(lambda_high, headers="keys", tablefmt="orgtbl", showindex=True)
#+end_src

#+RESULTS:
|              | mean |   sd | r_hat | target_value |
|--------------+------+------+-------+--------------|
| Lambda[2, 0] | 0.32 |  0.6 |  1.09 |            3 |
| Lambda[3, 1] |  0.4 | 0.57 |  1.13 |            2 |

** Convergence criteria

We compute the mean r_hat for each category of parameters.

#+begin_src python :tangle yes :comments both :results value raw :session :exports both
# Compute r_hat mean and std
rhat_alpha_mean = round(alpha_est["r_hat"].mean(), 2)
rhat_alpha_std = round(alpha_est["r_hat"].std(), 2)
rhat_beta_mean = round(beta_est["r_hat"].mean(), 2)
rhat_beta_std = round(beta_est["r_hat"].std(), 2)
rhat_W_mean = round(W_est["r_hat"].mean(), 2)
rhat_W_std = round(W_est["r_hat"].std(), 2)
rhat_lambda_mean = round(lambda_est["r_hat"].mean(), 2)
rhat_lambda_std = round(lambda_est["r_hat"].std(), 2)
rhat_lambda_diag_mean = round(lambda_est.loc[["Lambda[0, 0]", "Lambda[1, 1]"], ["r_hat"]]["r_hat"].mean(), 2)
rhat_lambda_diag_std = round(lambda_est.loc[["Lambda[0, 0]", "Lambda[1, 1]"], ["r_hat"]]["r_hat"].std(), 2)
rhat_lambda_high_mean = round(lambda_est.loc[["Lambda[2, 0]", "Lambda[3, 1]"], ["r_hat"]]["r_hat"].mean(), 2)
rhat_lambda_high_std = round(lambda_est.loc[["Lambda[2, 0]", "Lambda[3, 1]"], ["r_hat"]]["r_hat"].std(), 2)

# Build dataframe
par_names = ["alpha", "beta", "W", "lambda", "lambda_diag", "lambda_high"]
mean_val = [eval("rhat_" + x + "_mean") for x in par_names]
std_val = [eval("rhat_" + x + "_std") for x in par_names]

rhat_dic = {"par": par_names,
            "r_hat_mean": mean_val, "rhat_std": std_val}
rhat_df = pd.DataFrame(rhat_dic)
tabulate(rhat_df, headers="keys", tablefmt="orgtbl", showindex=False)
#+end_src

#+RESULTS:
| par         | r_hat_mean | rhat_std |
|-------------+------------+----------|
| alpha       |          1 |        0 |
| beta        |          1 |        0 |
| W           |       1.21 |     0.27 |
| lambda      |       1.24 |     0.32 |
| lambda_diag |          1 |        0 |
| lambda_high |       1.11 |     0.03 |


** Predicted vs. target parameter values

#+begin_src python :tangle yes :comments both :results output :session :exports both
# alpha
f = out_dir + "alpha.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(alpha_target, alpha_est["mean"], c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.set_title("alpha")
fig.savefig(f)

# beta
f = out_dir + "beta.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(beta_target.flatten(), beta_est["mean"], c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.set_title("beta")
fig.savefig(f)

# W_0
f = out_dir + "W_0.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(W_target[:, 0], W_0_est["mean"], c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.axline((-1, 1), slope=-1, ls="--", c=".3")
ax.set_title("W_0")
fig.savefig(f)

# W_1
f = out_dir + "W_1.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(W_target[:, 1], W_1_est["mean"], c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.axline((-1, 1), slope=-1, ls="--", c=".3")
ax.set_title("W_1")
fig.savefig(f)

# lambda_0
f = out_dir + "lambda_0.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(lambda_target[:, 0], lambda_0_est["mean"], c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.axline((-1, 1), slope=-1, ls="--", c=".3")
ax.set_title("lambda_0")
fig.savefig(f)

# lambda_1
f = out_dir + "lambda_1.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(lambda_target[:, 1], lambda_1_est["mean"], c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.axline((-1, 1), slope=-1, ls="--", c=".3")
ax.set_title("lambda_1")
fig.savefig(f)

# W_lambda
W_lambda_est = np.matmul(
    np.asarray(W_est["mean"]).reshape(n_sites, n_q),
    np.asarray(lambda_est["mean"]).reshape(n_species, n_q).transpose())
f = out_dir + "W_lambda.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(Wlambda_target.flatten(), W_lambda_est.flatten(), c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.set_title("W_lambda")
fig.savefig(f)
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_0.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_0.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_1.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_1.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "lambda_0.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/lambda_0.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "lambda_1.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/lambda_1.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_lambda.png")
#+end_src

#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_lambda.png]]

#+ATTR_HTML: :width 450

* Correcting for species order
** Sorting species

Species with high factor values are used for constraints.

#+begin_src python :tangle yes :comments both :results output :session :exports both
Y_sort = np.copy(Y)
Y_sort[:, 0] = Y[:, 2]
Y_sort[:, 1] = Y[:, 3]
Y_sort[:, 2] = Y[:, 0]
Y_sort[:, 3] = Y[:, 1]
Y = Y_sort
#+end_src

#+RESULTS:

** Statistical model

#+begin_src python :tangle yes :comments both :results output :session :exports both
with pm.Model() as model_sort:
    # Hyperpriors
    sigma_alpha = pm.HalfNormal("sigma_alpha", sigma=1.0)
    # Priors
    # Site random effect
    alpha = pm.Normal("alpha", mu=0, sigma=sigma_alpha, shape=n_sites, dims="sites")
    # Latent variables
    W = pm.Normal("W", mu=0, sigma=1, shape=(n_sites, n_q), dims=("sites", "latent_axis"))
    # Species effects
    beta = pm.Normal("beta", mu=0, sigma=1, shape=(n_species, n_p), dims=("species", "fixed_effects"))
    # Factor loadings with constraints
    Lambda = makeLambda(n_species, n_q, ("species", "latent_axis"))
    # Likelihood
    Xbeta = pm.math.dot(X, beta.transpose())
    Wlambda = pm.math.dot(W, Lambda.transpose()) 
    logit_theta = alpha[:, np.newaxis] + Xbeta + Wlambda
    obs = pm.Bernoulli("obs", logit_p=logit_theta, observed=Y)
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results silent :session :exports code
# Inference
with model_sort:
    trace_sort = pm.sample(**SAMPLE_KWARGS)
#+end_src

Save model with cloudpickle.

#+begin_src python :tangle yes :comments both :results silent :session :exports both
out_dir = "outputs/simulated-data-bad-constraints/"
with open(out_dir + "model_trace_sort.pkl", "wb") as f:
     model_trace_dump = cloudpickle.dumps({'model': model_sort, 'trace': trace_sort})
     f.write(model_trace_dump)
#+end_src

** Convergence and model performance

#+begin_src python :tangle yes :comments both :results output :session :exports code
with model_sort:
    alpha_est = az.summary(trace_sort, var_names=["alpha"], round_to=2)
    beta_est = az.summary(trace_sort, var_names=["beta"], round_to=2)
    lambda_est = az.summary(trace_sort, var_names=["Lambda"], round_to=2)
    lambda_0_est = az.summary(trace_sort,
                              var_names=["Lambda"],
                              coords={"latent_axis": [0]},
                              round_to=2)
    lambda_1_est = az.summary(trace_sort,
                              var_names=["Lambda"],
                              coords={"latent_axis": [1]},
                              round_to=2)
    W_est = az.summary(trace_sort, var_names=["W"], round_to=2)
    W_0_est = az.summary(trace_sort, var_names=["W"],
                         coords={"latent_axis": [0]},
                         round_to=2)
    W_1_est = az.summary(trace_sort, var_names=["W"],
                         coords={"latent_axis": [1]},
                         round_to=2)
#+end_src

#+RESULTS:
: /home/ghislain/.pyenv/versions/miniconda3-latest/envs/JSDM-PyMC/lib/python3.9/site-packages/arviz/stats/diagnostics.py:584: RuntimeWarning: invalid value encountered in scalar divide
:   (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)
: /home/ghislain/.pyenv/versions/miniconda3-latest/envs/JSDM-PyMC/lib/python3.9/site-packages/arviz/stats/diagnostics.py:584: RuntimeWarning: invalid value encountered in scalar divide
:   (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)

#+begin_src python :tangle yes :comments both :results value raw :session :exports both
lambda_diag = lambda_est.loc[["Lambda[0, 0]", "Lambda[1, 1]"], ["mean", "sd", "r_hat"]]
lambda_diag["target_value"] = [lambda_target[2, 0], lambda_target[3, 1]]
col_names = ["Distance", "Npixels", "Area", "Cumulation", "Percentage"]
tabulate(lambda_diag, headers="keys", tablefmt="orgtbl", showindex=True)
#+end_src

#+RESULTS:
|              | mean |   sd | r_hat | target_value |
|--------------+------+------+-------+--------------|
| Lambda[0, 0] | 0.98 | 0.39 |  1.81 |            3 |
| Lambda[1, 1] | 0.87 | 0.17 |   1.1 |            2 |

#+begin_src python :tangle yes :comments both :results value raw :session :exports both
# Compute r_hat mean and std
rhat_alpha_mean = round(alpha_est["r_hat"].mean(), 2)
rhat_alpha_std = round(alpha_est["r_hat"].std(), 2)
rhat_beta_mean = round(beta_est["r_hat"].mean(), 2)
rhat_beta_std = round(beta_est["r_hat"].std(), 2)
rhat_W_mean = round(W_est["r_hat"].mean(), 2)
rhat_W_std = round(W_est["r_hat"].std(), 2)
rhat_lambda_mean = round(lambda_est["r_hat"].mean(), 2)
rhat_lambda_std = round(lambda_est["r_hat"].std(), 2)
rhat_lambda_diag_mean = round(lambda_est.loc[["Lambda[0, 0]", "Lambda[1, 1]"], ["r_hat"]]["r_hat"].mean(), 2)
rhat_lambda_diag_std = round(lambda_est.loc[["Lambda[0, 0]", "Lambda[1, 1]"], ["r_hat"]]["r_hat"].std(), 2)
rhat_lambda_small_mean = round(lambda_est.loc[["Lambda[2, 0]", "Lambda[3, 1]"], ["r_hat"]]["r_hat"].mean(), 2)
rhat_lambda_small_std = round(lambda_est.loc[["Lambda[2, 0]", "Lambda[3, 1]"], ["r_hat"]]["r_hat"].std(), 2)

# Build dataframe
par_names = ["alpha", "beta", "W", "lambda", "lambda_diag", "lambda_small"]
mean_val = [eval("rhat_" + x + "_mean") for x in par_names]
std_val = [eval("rhat_" + x + "_std") for x in par_names]

rhat_dic = {"par": par_names,
            "r_hat_mean": mean_val, "rhat_std": std_val}
rhat_df = pd.DataFrame(rhat_dic)
tabulate(rhat_df, headers="keys", tablefmt="orgtbl", showindex=False)
#+end_src

#+RESULTS:
| par          | r_hat_mean | rhat_std |
|--------------+------------+----------|
| alpha        |          1 |        0 |
| beta         |          1 |        0 |
| W            |       1.33 |     0.25 |
| lambda       |       1.45 |     0.31 |
| lambda_diag  |       1.46 |      0.5 |
| lambda_small |       1.02 |     0.02 |

** Predicted vs. target parameter values

#+begin_src python :tangle yes :comments both :results output :session :exports both
# Sorted index
id = [2, 3, 0, 1] + list(np.arange(4, n_species))

# alpha
f = out_dir + "alpha_sort.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(alpha_target, alpha_est["mean"], c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.set_title("alpha_sort")
fig.savefig(f)

# beta
f = out_dir + "beta_sort.png"
fig, ax = plt.subplots(figsize=(6, 6))
beta_hat = np.asarray(beta_est["mean"]).reshape(n_species, n_p)[id, :]
ax.scatter(beta_target.flatten(), beta_hat.flatten(), c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.set_title("beta_sort")
fig.savefig(f)

# W_0
f = out_dir + "W_0_sort.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(W_target[:, 0], W_0_est["mean"], c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.axline((-1, 1), slope=-1, ls="--", c=".3")
ax.set_title("W_0_sort")
fig.savefig(f)

# W_1
f = out_dir + "W_1_sort.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(W_target[:, 1], W_1_est["mean"], c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.axline((-1, 1), slope=-1, ls="--", c=".3")
ax.set_title("W_1_sort")
fig.savefig(f)

# lambda_0
f = out_dir + "lambda_0_sort.png"
fig, ax = plt.subplots(figsize=(6, 6))
lambda_0_hat = lambda_0_est["mean"][id]
ax.scatter(lambda_target[:, 0], lambda_0_hat, c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.axline((-1, 1), slope=-1, ls="--", c=".3")
ax.set_title("lambda_0_sort")
fig.savefig(f)

# lambda_1
f = out_dir + "lambda_1_sort.png"
fig, ax = plt.subplots(figsize=(6, 6))
lambda_1_hat = lambda_1_est["mean"][id]
ax.scatter(lambda_target[:, 1], lambda_1_hat, c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.axline((-1, 1), slope=-1, ls="--", c=".3")
ax.set_title("lambda_1_sort")
fig.savefig(f)

# W_lambda
lambda_hat = np.asarray(lambda_est["mean"]).reshape(n_species, n_q)[id, :]
W_lambda_est = np.matmul(
    np.asarray(W_est["mean"]).reshape(n_sites, n_q),
    lambda_hat.transpose())
f = out_dir + "W_lambda_sort.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(Wlambda_target.flatten(), W_lambda_est.flatten(), c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.set_title("W_lambda_sort")
fig.savefig(f)
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_0.png")
#+end_src

#+ATTR_HTML: :width 450 :style float:left;
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_0.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_0_sort.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_0_sort.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_1.png")
#+end_src

#+ATTR_HTML: :width 450 :style float:left;
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_1.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_1_sort.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_1_sort.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "lambda_0.png")
#+end_src

#+ATTR_HTML: :width 450 :style float:left;
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/lambda_0.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "lambda_0_sort.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/lambda_0_sort.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "lambda_1.png")
#+end_src

#+ATTR_HTML: :width 450 :style float:left;
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/lambda_1.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "lambda_1_sort.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/lambda_1_sort.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_lambda.png")
#+end_src

#+ATTR_HTML: :width 450 :style float:left;
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_lambda.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_lambda_sort.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_lambda_sort.png]]

* Automatic sorting of species with PCAÂ on residuals

** Unsorted data

#+begin_src python :tangle yes :comments both :results output :session :exports both
f = open(out_dir + "data.pkl", "rb")
data = cloudpickle.loads(f.read())
Y = data["Y"]
X = data["X"]
print("X.shape:")
print(X.shape)
print("\nX[:5,]:")
print(X[:5,])
print("\nY.shape:")
print(Y.shape)
#+end_src

#+RESULTS:
#+begin_example
X.shape:
(100, 3)

X[:5,]:
[[ 1.         -1.60383681  2.25392546]
 [ 1.          0.06409991  0.1616142 ]
 [ 1.          0.7408913   0.83377881]
 [ 1.          0.15261919 -1.58010947]
 [ 1.          0.86374389  1.01058529]]

Y.shape:
(100, 30)
#+end_example


** Statistical model with residuals

#+begin_src python :tangle yes :comments both :results output :session :exports both
with pm.Model() as model_res:
    # Hyperpriors
    sigma_alpha = pm.HalfNormal("sigma_alpha", sigma=1.0)
    # Priors
    # Site random effect
    alpha = pm.Normal("alpha", mu=0, sigma=sigma_alpha, shape=n_sites)
    # Species effects
    beta = pm.Normal("beta", mu=0, sigma=1, shape=(n_species, n_p))
    # Likelihood
    Xbeta = pm.math.dot(X, beta.transpose())
    m = pm.Deterministic("mu", alpha[:, np.newaxis] + Xbeta)
    logit_theta = pm.Normal("logit_theta", mu=m, sigma=1)
    e = pm.Deterministic("error", logit_theta - m)
    obs = pm.Bernoulli("obs", logit_p=logit_theta, observed=Y)
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results silent :session :exports code
# Inference
with model_res:
    trace_res = pm.sample(**SAMPLE_KWARGS)
#+end_src

Save model with cloudpickle.

#+begin_src python :tangle yes :comments both :results silent :session :exports both
with open(out_dir + "model_trace_res.pkl", "wb") as f:
     model_trace_dump = cloudpickle.dumps({'model': model_res, 'trace': trace_res})
     f.write(model_trace_dump)
#+end_src

Get residuals.

#+begin_src python :tangle yes :comments both :results output :session :exports both
with model_res:
    error_est = az.summary(trace_res, var_names=["error"], round_to=2)
e = np.asarray(error_est["mean"]).reshape(n_sites, n_species)
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results file :session :exports both
ofile = os.path.join(out_dir, "hist_residuals.png")
fig = plt.figure()
plt.hist(e.flatten(), bins=20)
fig.savefig(ofile)
ofile
#+end_src

#+ATTR_HTML: :width 900
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/hist_residuals.png]]

Correlation between residuals and Wlambda.

#+begin_src python :tangle yes :comments both :results file :session :exports both
ofile = out_dir + "corr_res_Wlambda.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(Wlambda_target, e, c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.set_xlabel("Wlambda_target")
ax.set_ylabel("Estimated residuals")
ax.set_title("corr_res_Wlambda")
fig.savefig(ofile)
ofile
#+end_src

#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/corr_res_Wlambda.png]]

** PCA on residuals

Make the PCA on residuals to find the coordinates of the species on two axis.

#+begin_src python :tangle yes :comments both :results output :session :exports both
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

pca = PCA(n_components=2)
e_cr = StandardScaler().fit_transform(e)
pca_features = pca.fit_transform(e_cr)
pca_features.shape
print(pca.explained_variance_ratio_)
#+end_src

#+RESULTS:
: [0.1473642  0.08463718]

Here, the first axis explains 15% of the inertia while the second axis explains only about half (8%).

#+begin_src python :tangle yes :comments both :results output :session :exports both
pca_comp = pca.components_.transpose()
print(pca_comp)
#+end_src

#+RESULTS:
#+begin_example
[[ 0.03396599 -0.03963534]
 [ 0.08106467 -0.01943345]
 [ 0.34287058 -0.07872585]
 [ 0.09137384  0.41863059]
 [-0.03692905 -0.230375  ]
 [ 0.07233174  0.17641527]
 [ 0.34182926 -0.00529459]
 [ 0.23940789  0.19472885]
 [ 0.11293715 -0.11895565]
 [-0.26320691  0.25605415]
 [ 0.18673312  0.16947049]
 [-0.20999865 -0.24288086]
 [ 0.01866028 -0.33128662]
 [-0.26018591  0.15926302]
 [-0.1778047   0.15828194]
 [-0.2654631  -0.08457957]
 [-0.2336753  -0.08578012]
 [-0.01329867 -0.07962129]
 [-0.19177979  0.01812773]
 [ 0.00672282  0.28193292]
 [ 0.20784002  0.1169296 ]
 [ 0.16194043  0.01727679]
 [ 0.23487854 -0.080573  ]
 [-0.03197814 -0.02116663]
 [-0.12896861  0.28435715]
 [ 0.03015344  0.02176053]
 [-0.18236438 -0.10136769]
 [-0.16227703 -0.09967419]
 [-0.0371788  -0.28519601]
 [-0.24236659  0.24981696]]
#+end_example

Identify the species which influences most each component.

#+begin_src python :tangle yes :comments both :results output :session :exports both
pca_comp_abs = np.abs(pca_comp)
sp_sel = np.argmax(pca_comp_abs, axis=0)
print(sp_sel)
#+end_src

#+RESULTS:
: [2 3]

We correctly identified the two species. We look again at the factor loadings for these two species.

#+begin_src python :tangle yes :comments both :results output :session :exports both
print(lambda_target[sp_sel, :])
#+end_src

#+RESULTS:
: [[3.         0.        ]
:  [0.07563471 2.        ]]

Sorting species.

#+begin_src python :tangle yes :comments both :results output :session :exports both
Y_sort = np.copy(Y)
Y_sort[:, 0] = Y[:, sp_sel[0]]
Y_sort[:, 1] = Y[:, sp_sel[1]]
Y_sort[:, sp_sel[0]] = Y[:, 0]
Y_sort[:, sp_sel[1]] = Y[:, 1]
Y = Y_sort
#+end_src

#+RESULTS:

** Correlation between factor loadings and species coordinates on the two axis of the PCA

#+begin_src python :tangle yes :comments both :results output :session :exports both
cor = np.corrcoef(np.abs(lambda_target.flatten()), np.abs(pca_comp.flatten()))
print(cor)
#+end_src

#+RESULTS:
: [[1.         0.76732233]
:  [0.76732233 1.        ]]

#+begin_src python :tangle yes :comments both :results file :session :exports both
ofile = os.path.join(out_dir, "cor_lambda_coordPCA_e.png")
fig, ax = plt.subplots()
ax.scatter(np.abs(lambda_target).flatten(), np.abs(pca_comp).flatten())
ax.set(xlabel="lambda targets", ylabel="Coordinates on PCA axis")
fig.savefig(ofile)
ofile
#+end_src

#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/cor_lambda_coordPCA_e.png]]
 
** Statistical model with sorted species

#+begin_src python :tangle yes :comments both :results output :session :exports both
with pm.Model() as model_auto:
    # Hyperpriors
    sigma_alpha = pm.HalfNormal("sigma_alpha", sigma=1.0)
    # Priors
    # Site random effect
    alpha = pm.Normal("alpha", mu=0, sigma=sigma_alpha, shape=n_sites)
    # Latent variables
    W = pm.Normal("W", mu=0, sigma=1, shape=(n_sites, n_q))
    # Species effects
    beta = pm.Normal("beta", mu=0, sigma=1, shape=(n_species, n_p))
    # Factor loadings with constraints
    # Diagonal
    Lambda1 = pt.set_subtensor(
        Lambda0[np.arange(n_q), np.arange(n_q)],
        pm.HalfNormal("Lambda_diag", sigma=HALFNORMAL_SCALE, shape=n_q))
    # Inferior
    Lambda2 = pt.set_subtensor(
        Lambda1[1, 0],
        pm.Normal("Lambda_inf", mu=0, sigma=1))
    # Block
    Lambda = pm.Deterministic(
        "Lambda",
        pt.set_subtensor(
            Lambda2[n_q:],
            pm.Normal("Lambda_block", mu=0, sigma=1, shape=(n_species-n_q, n_q))))
    # Likelihood
    Xbeta = pm.math.dot(X, beta.transpose())
    Wlambda = pm.math.dot(W, Lambda.transpose()) 
    logit_theta = alpha[:, np.newaxis] + Xbeta + Wlambda
    obs = pm.Bernoulli("obs", logit_p=logit_theta, observed=Y)
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results silent :session :exports code
# Inference
with model_auto:
    trace_auto = pm.sample(**SAMPLE_KWARGS)
#+end_src

Save model with cloudpickle.

#+begin_src python :tangle yes :comments both :results silent :session :exports both
out_dir = "outputs/simulated-data-bad-constraints/"
with open(out_dir + "model_trace_auto.pkl", "wb") as f:
     model_trace_dump = cloudpickle.dumps({'model': model_auto, 'trace': trace_auto})
     f.write(model_trace_dump)
#+end_src

** Convergence and model performance

#+begin_src python :tangle yes :comments both :results output :session :exports code
with model_auto:
    alpha_est = az.summary(trace_auto, var_names=["alpha"], round_to=2)
    beta_est = az.summary(trace_auto, var_names=["beta"], round_to=2)
    lambda_est = az.summary(trace_auto, var_names=["Lambda"], round_to=2)
    lambda_0_est = az.summary(trace_auto,
                              var_names=["Lambda"],
                              coords={"Lambda_dim_1": [0]},
                              round_to=2)
    lambda_1_est = az.summary(trace_auto,
                              var_names=["Lambda"],
                              coords={"Lambda_dim_1": [1]},
                              round_to=2)
    W_est = az.summary(trace_auto, var_names=["W"], round_to=2)
    W_0_est = az.summary(trace_auto, var_names=["W"],
                         coords={"W_dim_1": [0]},
                         round_to=2)
    W_1_est = az.summary(trace_auto, var_names=["W"],
                         coords={"W_dim_1": [1]},
                         round_to=2)
#+end_src

#+RESULTS:
: /home/ghislain/.pyenv/versions/miniconda3-latest/envs/JSDM-PyMC/lib/python3.9/site-packages/arviz/stats/diagnostics.py:584: RuntimeWarning: invalid value encountered in scalar divide
:   (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)
: /home/ghislain/.pyenv/versions/miniconda3-latest/envs/JSDM-PyMC/lib/python3.9/site-packages/arviz/stats/diagnostics.py:584: RuntimeWarning: invalid value encountered in scalar divide
:   (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)

#+begin_src python :tangle yes :comments both :results value raw :session :exports both
# Compute r_hat mean and std
rhat_alpha_mean = round(alpha_est["r_hat"].mean(), 2)
rhat_alpha_std = round(alpha_est["r_hat"].std(), 2)
rhat_beta_mean = round(beta_est["r_hat"].mean(), 2)
rhat_beta_std = round(beta_est["r_hat"].std(), 2)
rhat_W_mean = round(W_est["r_hat"].mean(), 2)
rhat_W_std = round(W_est["r_hat"].std(), 2)
rhat_lambda_mean = round(lambda_est["r_hat"].mean(), 2)
rhat_lambda_std = round(lambda_est["r_hat"].std(), 2)
rhat_lambda_diag_mean = round(lambda_est.loc[["Lambda[0, 0]", "Lambda[1, 1]"], ["r_hat"]]["r_hat"].mean(), 2)
rhat_lambda_diag_std = round(lambda_est.loc[["Lambda[0, 0]", "Lambda[1, 1]"], ["r_hat"]]["r_hat"].std(), 2)
rhat_lambda_small_mean = round(lambda_est.loc[["Lambda[2, 0]", "Lambda[3, 1]"], ["r_hat"]]["r_hat"].mean(), 2)
rhat_lambda_small_std = round(lambda_est.loc[["Lambda[2, 0]", "Lambda[3, 1]"], ["r_hat"]]["r_hat"].std(), 2)

# Build dataframe
par_names = ["alpha", "beta", "W", "lambda", "lambda_diag", "lambda_small"]
mean_val = [eval("rhat_" + x + "_mean") for x in par_names]
std_val = [eval("rhat_" + x + "_std") for x in par_names]
rhat_dic = {"par": par_names,
            "r_hat_mean": mean_val, "rhat_std": std_val}
rhat_df = pd.DataFrame(rhat_dic)
tabulate(rhat_df, headers="keys", tablefmt="orgtbl", showindex=False)
#+end_src

#+RESULTS:

** Predicted vs. target parameter values

Caution, the latent axis $W_i$ are inverted here.

#+begin_src python :tangle yes :comments both :results output :session :exports both
# Sorted index
id = np.arange(n_species)
id_sort = np.copy(id)
id_sort[0] = sp_sel[0]
id_sort[1] = sp_sel[1]
id_sort[sp_sel[0]] = 0
id_sort[sp_sel[1]] = 1
id = id_sort

# alpha
f = out_dir + "alpha_auto.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(alpha_target, alpha_est["mean"], c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.set_title("alpha_auto")
fig.savefig(f)

# beta
f = out_dir + "beta_auto.png"
fig, ax = plt.subplots(figsize=(6, 6))
beta_hat = np.asarray(beta_est["mean"]).reshape(n_species, n_p)[id, :]
ax.scatter(beta_target.flatten(), beta_hat.flatten(), c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.set_title("beta_auto")
fig.savefig(f)

# W_0
f = out_dir + "W_0_auto.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(W_target[:, 0], W_0_est["mean"], c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.axline((-1, 1), slope=-1, ls="--", c=".3")
ax.set_title("W_0_auto")
fig.savefig(f)

# W_1
f = out_dir + "W_1_auto.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(W_target[:, 1], W_1_est["mean"], c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.axline((-1, 1), slope=-1, ls="--", c=".3")
ax.set_title("W_1_auto")
fig.savefig(f)

# lambda_0
f = out_dir + "lambda_0_auto.png"
fig, ax = plt.subplots(figsize=(6, 6))
lambda_0_hat = lambda_0_est["mean"][id]
ax.scatter(lambda_target[:, 0], lambda_0_hat, c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.axline((-1, 1), slope=-1, ls="--", c=".3")
ax.set_title("lambda_0_auto")
fig.savefig(f)

# lambda_1
f = out_dir + "lambda_1_auto.png"
fig, ax = plt.subplots(figsize=(6, 6))
lambda_1_hat = lambda_1_est["mean"][id]
ax.scatter(lambda_target[:, 1], lambda_1_hat, c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.axline((-1, 1), slope=-1, ls="--", c=".3")
ax.set_title("lambda_1_auto")
fig.savefig(f)

# W_lambda
lambda_hat = np.asarray(lambda_est["mean"]).reshape(n_species, n_q)[id, :]
W_lambda_est = np.matmul(
    np.asarray(W_est["mean"]).reshape(n_sites, n_q),
    lambda_hat.transpose())
f = out_dir + "W_lambda_auto.png"
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(Wlambda_target.flatten(), W_lambda_est.flatten(), c=".3")
ax.axline((1, 1), slope=1, ls="--", c=".3")
ax.set_title("W_lambda_auto")
fig.savefig(f)
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_0.png")
#+end_src

#+ATTR_HTML: :width 450 :style float:left;
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_0.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_0_auto.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_0_auto.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_1.png")
#+end_src

#+ATTR_HTML: :width 450 :style float:left;
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_1.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_1_auto.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_1_auto.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "lambda_0.png")
#+end_src

#+ATTR_HTML: :width 450 :style float:left;
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/lambda_0.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "lambda_0_auto.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/lambda_0_auto.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "lambda_1.png")
#+end_src

#+ATTR_HTML: :width 450 :style float:left;
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/lambda_1.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "lambda_1_auto.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/lambda_1_auto.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_lambda.png")
#+end_src

#+ATTR_HTML: :width 450 :style float:left;
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_lambda.png]]

#+begin_src python :tangle yes :comments both :results file :session :exports results
os.path.join(out_dir, "W_lambda_auto.png")
#+end_src

#+ATTR_HTML: :width 450
#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/W_lambda_auto.png]]

* Randomized quantile residuals from GLMM

*DOES NOT SEEM TO WORK**

** Fitting a classical GLMM

Install [[https://github.com/fabsig/GPBoost][gpboost]].

#+begin_src shell
conda activate JSDM-PyMC
pip install gpboost -U
#+end_src

Import the libary.

#+begin_src python :tangle yes :comments both :results output :session :exports both
import gpboost as gpb
#+end_src

#+RESULTS:

Make the grouping variable (sites) for random site effects.

#+begin_src python :tangle yes :comments both :results output :session :exports both
group_data = np.array(["Site_" + "{:03d}".format(x) for x in (list(range(1, n_sites + 1)) * n_species)])
#+end_src

#+RESULTS:

Make the X matrix of explanatory variables.

#+begin_src python :tangle yes :comments both :results output :session :exports both
# n
n_obs = n_sites * n_species
n_par = n_p * n_species
# X_large
X_long = np.array(list(X.flatten()) * n_species).reshape(n_obs, n_p)
X_large = np.array([y for x in X_long.flatten() for y in [x] * n_species]).reshape(n_obs, n_par)
# Sp_large
Sp = ["Sp_" + "{:02d}".format(y) for x in range(1, n_species + 1) for y in [x] * n_sites]
Sp_design = np.array(pd.Series(Sp).str.get_dummies())
Sp_large = np.array(list(Sp_design.flatten("F")) * n_p).reshape(n_par, n_obs).transpose()
# X_design
X_design = X_large * Sp_large
#+end_src

#+RESULTS:

Make the response variable in long format.

#+begin_src python :tangle yes :comments both :results output :session :exports both
y = Y.flatten(order="F")
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results output :session :exports both
gp_model = gpb.GPModel(group_data=group_data, likelihood="binary", seed=12345)
gp_model.fit(y=y, X=X_design)
gp_model.summary()
#+end_src

#+RESULTS:
#+begin_example
[GPBoost] [Warning] The covariate data contains no column of ones, i.e., no intercept is included.
=====================================================
Model summary:
 Log-lik     AIC     BIC
-1795.82 3773.64 4320.22
Nb. observations: 3000
Nb. groups: 100 (Group_1)
-----------------------------------------------------
Covariance parameters (random effects):
         Param.
Group_1  0.0905
-----------------------------------------------------
Linear regression coefficients (fixed effects):
              Param.
Covariate_1  -0.4852
Covariate_2  -0.2379
Covariate_3   0.3104
Covariate_4  -0.1321
Covariate_5  -0.0774
...              ...
Covariate_86  0.4797
Covariate_87 -0.3199
Covariate_88 -0.3297
Covariate_89  0.3621
Covariate_90 -0.4063

[90 rows x 1 columns]
=====================================================
#+end_example

Get coefficients.

#+begin_src python :tangle yes :comments both :results output :session :exports both
coefs = gp_model.get_coef()
#+end_src

#+RESULTS:

Plot estimated vs. target coefficients.

#+begin_src python :tangle yes :comments both :results output :session :exports both
import matplotlib.pyplot as plt
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results file :session :exports both
ofile = os.path.join(out_dir, "beta_est_target_gp.png")
fig, ax = plt.subplots()
ax.scatter(np.array(coefs.iloc[0]), beta_target.flatten("F"))
ax.set(xlim=(-1, 1),
       ylim=(-1, 1),
       xlabel="Estimated betas",
       ylabel="Target betas",
       aspect=1)
ax.axline((1, 1), slope=1, ls="--", c=".3")
fig.tight_layout()
fig.savefig(ofile)
ofile
#+end_src

#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/beta_est_target_gp.png]]

** Computing randomized quantile residuals

#+begin_src python :tangle yes :comments both :results output :session :exports both
import scipy
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results output :session :exports both
pred = gp_model.predict(X_pred=X_design, group_data_pred=group_data,
                        predict_var=False, predict_response=False)
probit_theta_hat = pred["mu"]
theta_hat = scipy.stats.norm.cdf(probit_theta_hat)
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results output :session :exports both
# Random draws
np.random.seed(4321)
rho = np.random.uniform(low=0, high=1, size=n_obs)
# Quantile residuals
quantres = np.zeros(n_obs)
quantres[y==0] = scipy.stats.norm.ppf(rho[y==0] * (1 - theta_hat[y==0]))
quantres[y==1] = scipy.stats.norm.ppf(1 - theta_hat[y==1] + rho[y==1] * theta_hat[y==1])
#+end_src

#+RESULTS:

#+begin_src python :tangle yes :comments both :results file :session :exports both
#+begin_src python :tangle yes :comments both :results file :session :exports both
ofile = os.path.join(out_dir, "hist_quantres.png")
fig = plt.figure()
plt.hist(quantres, bins=20)
fig.savefig(ofile)
ofile
#+end_src

#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/hist_quantres.png]]

#+end_src

** PCA on quantile residuals

Make the PCA on residuals to find the coordinates of the species on two axis.

#+begin_src python :tangle yes :comments both :results output :session :exports both
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Residuals by site and species
quantres_wide = quantres.reshape(n_species, n_sites).transpose()

pca = PCA(n_components=2)
e_cr = StandardScaler().fit_transform(quantres_wide)
pca_features = pca.fit_transform(e_cr)
pca_features.shape
print(pca.explained_variance_ratio_)
#+end_src

#+RESULTS:
: [0.1125634 0.0733407]

Here, the first axis explains 11% of the inertia while the second axis explains only about half (6%).

#+begin_src python :tangle yes :comments both :results output :session :exports both
pca_comp = pca.components_.transpose()
print(pca_comp)
#+end_src

#+RESULTS:
#+begin_example
[[ 0.27400444  0.01796886]
 [ 0.05301124  0.34720178]
 [-0.10243455  0.14314267]
 [ 0.05116339  0.01546719]
 [-0.03737999 -0.17679844]
 [-0.01150649  0.28733574]
 [ 0.39537859  0.03532233]
 [ 0.30757504  0.09323177]
 [ 0.12231542 -0.27067824]
 [-0.25525717  0.21967261]
 [ 0.14139182 -0.08324606]
 [-0.22606147 -0.1812116 ]
 [ 0.05872837 -0.28127176]
 [-0.31373848  0.08014922]
 [-0.27094273  0.08571908]
 [-0.27549154 -0.14099574]
 [-0.19316758 -0.19381134]
 [ 0.02594146 -0.06983447]
 [-0.11006644  0.1447955 ]
 [ 0.02281929  0.33149899]
 [ 0.10819291  0.17008849]
 [ 0.17124614  0.1197997 ]
 [ 0.30155877 -0.08864229]
 [ 0.0518107  -0.14810316]
 [-0.04848178  0.2112555 ]
 [-0.04154337  0.15547466]
 [-0.14722086  0.00726332]
 [-0.10635143 -0.13305797]
 [-0.10175069 -0.30115877]
 [-0.14647964  0.1922445 ]]
#+end_example

From the coordinates on the two axis of the PCA, we clearly see that the first two species do not contribute much to the two axis (coordinates of -0.074 and -0.079).

Identify the species which influences most each component.

#+begin_src python :tangle yes :comments both :results output :session :exports both
pca_comp_abs = np.abs(pca_comp)
sp_sel = np.argmax(pca_comp_abs, axis=0)
print(sp_sel)
#+end_src

#+RESULTS:
: [6 1]

We look again at the factor loadings for these two species.

#+begin_src python :tangle yes :comments both :results output :session :exports both
print(lambda_target[sp_sel, :])
#+end_src

#+RESULTS:
: [[ 1.74099277  0.43056231]
:  [ 0.14983372 -0.1       ]]

There are higher than for the two default species.

** Correlation between factor loadings and species coordinates on the two axis of the PCA

#+begin_src python :tangle yes :comments both :results output :session :exports both
cor = np.corrcoef(np.abs(lambda_target.flatten()), np.abs(pca_comp.flatten()))
print(cor)
#+end_src

#+RESULTS:
: [[1.         0.35742488]
:  [0.35742488 1.        ]]

#+begin_src python :tangle yes :comments both :results file :session :exports both
ofile = os.path.join(out_dir, "cor_lambda_coordPCA_qres.png")
fig, ax = plt.subplots()
ax.scatter(np.abs(lambda_target).flatten(), np.abs(pca_comp).flatten())
fig.savefig(ofile)
ofile
#+end_src

#+RESULTS:
[[file:outputs/simulated-data-bad-constraints/cor_lambda_coordPCA_qres.png]]

* Randomized quantile residuals from Bayesian model without latent variables

To do...

* Environment setup and test :noexport:

#+BEGIN_SRC python :tangle yes :comments both :results value
import sys
return sys.executable
#+END_SRC

#+RESULTS:
: /home/ghislain/.pyenv/versions/miniconda3-latest/envs/JSDM-PyMC/bin/python

# EOF
